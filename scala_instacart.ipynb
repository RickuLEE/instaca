{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Point: SparkSession\n",
    "The entry point into all functionality in Spark is the SparkSession class. To create a basic SparkSession, just use SparkSession.builder():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@434f30ef\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@434f30ef"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL instacart example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4.4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aisles = [aisle_id: int, aisle: string]\n",
       "departments = [department_id: int, department: string]\n",
       "order_products_prior = [order_id: int, product_id: int ... 2 more fields]\n",
       "order_products_train = [order_id: int, product_id: int ... 2 more fields]\n",
       "orders = [order_id: int, user_id: int ... 5 more fields]\n",
       "products = [product_id: int, product_name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[product_id: int, product_name: string ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aisles = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"aisles.csv\")\n",
    "val departments = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"departments.csv\")\n",
    "val order_products_prior = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"order_products__prior.csv\")\n",
    "val order_products_train = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"order_products__train.csv\")\n",
    "val orders = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"orders.csv\")\n",
    "val products= spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"products.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running SQL Queries Programmatically\n",
    "The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Register the DataFrame as a global temporary view\n",
    "// Global temporary view is tied to a system preserved database `global_temp`\n",
    "// Global temporary view is cross-session\n",
    "aisles.createOrReplaceTempView(\"aisles\")\n",
    "departments.createOrReplaceTempView(\"departments\")\n",
    "order_products_prior.createOrReplaceTempView(\"order_products_prior\")\n",
    "order_products_train.createOrReplaceTempView(\"order_products_train\")\n",
    "orders.createOrReplaceTempView(\"orders\")\n",
    "products.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|order_id|               items|\n",
      "+--------+--------------------+\n",
      "|    1342|[Raw Shrimp, Seed...|\n",
      "|    1591|[Cracked Wheat, S...|\n",
      "|    4519|[Beet Apple Carro...|\n",
      "+--------+--------------------+\n",
      "only showing top 3 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawData = [product_name: string, order_id: int]\n",
       "baskets = [order_id: int, items: array<string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, items: array<string>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Organize the data by shopping basket\n",
    "import org.apache.spark.sql.functions.{collect_set,col,count}\n",
    "val rawData = spark.sql(\"\"\"\n",
    "select p.product_name, o.order_id \n",
    "from products p \n",
    "inner join order_products_train o \n",
    "where o.product_id = p.product_id\"\"\")\n",
    "val baskets = rawData.groupBy(\"order_id\").agg(collect_set(\"product_name\").alias(\"items\"))\n",
    "baskets.createOrReplaceTempView(\"baskets\")\n",
    "baskets.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ML Model\n",
    "To understand the frequency of items are associated with each other (e.g. peanut butter and jelly), we will use association rule mining for market basket analysis. [Spark MLlib](http://spark.apache.org/docs/latest/mllib-guide.html) implements two algorithms related to frequency pattern mining (FPM): [FP-growth](https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html#fp-growth) and [PrefixSpan](https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html#prefixspan). The distinction is that FP-growth does not use order information in the itemsets, if any, while PrefixSpan is designed for sequential pattern mining where the itemsets are ordered. We will use FP-growth as the order information is not important for this use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we will be using the ```Scala API``` so we can configure ```setMinConfidence```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use FP-growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baskets_ds = [items: array<string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[items: array<string>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Extract out the items \n",
    "val baskets_ds = spark.sql(\"select items from baskets\").as[Array[String]].toDF(\"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fpgrowth = fpgrowth_aa0bdac30b6d\n",
       "model = fpgrowth_aa0bdac30b6d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fpgrowth_aa0bdac30b6d"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.fpm.FPGrowth\n",
    "// Use FPGrowth\n",
    "val fpgrowth = new FPGrowth().setItemsCol(\"items\").setMinSupport(0.001).setMinConfidence(0)\n",
    "val model = fpgrowth.fit(baskets_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Frequent Itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostPopularItemInABasket = [items: array<string>, freq: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[items: array<string>, freq: bigint]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Display frequent itemsets\n",
    "val mostPopularItemInABasket = model.freqItemsets\n",
    "mostPopularItemInABasket.createOrReplaceTempView(\"mostPopularItemInABasket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               items|freq|\n",
      "+--------------------+----+\n",
      "|[Organic Hass Avo...| 710|\n",
      "|[Organic Raspberr...| 649|\n",
      "|[Organic Baby Spi...| 587|\n",
      "|[Organic Raspberr...| 531|\n",
      "|[Organic Hass Avo...| 497|\n",
      "|[Organic Avocado,...| 484|\n",
      "|[Organic Avocado,...| 477|\n",
      "|[Limes, Large Lem...| 452|\n",
      "|[Organic Cucumber...| 424|\n",
      "|[Limes, Organic A...| 389|\n",
      "|[Organic Raspberr...| 381|\n",
      "|[Organic Avocado,...| 379|\n",
      "|[Organic Baby Spi...| 376|\n",
      "|[Organic Blueberr...| 374|\n",
      "|[Large Lemon, Org...| 371|\n",
      "|[Organic Cucumber...| 366|\n",
      "|[Organic Lemon, O...| 353|\n",
      "|[Limes, Organic A...| 352|\n",
      "|[Organic Whole Mi...| 339|\n",
      "|[Organic Avocado,...| 334|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlDF = [items: array<string>, freq: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[items: array<string>, freq: bigint]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlDF = spark.sql(\"select items, freq from mostPopularItemInABasket where size(items) > 2 order by freq desc limit 20\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Association Rules\n",
    "In addition to ```freqItemSets```, the ```FP-growth model``` also generates ```association rules```. For example, if a shopper purchases peanut butter , what is the likelihood that they will also purchase jelly. For more information, a good reference is Susan Li's [A Gentle Introduction on Market Basket Analysis — Association Rules](https://towardsdatascience.com/a-gentle-introduction-on-market-basket-analysis-association-rules-fa4b986a40ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Generated Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ifThen = [antecedent: array<string>, consequent: array<string> ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[antecedent: array<string>, consequent: array<string> ... 2 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Display generated association rules.\n",
    "val ifThen = model.associationRules\n",
    "ifThen.createOrReplaceTempView(\"ifThen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|     antecedent (if)|   consequent (then)|         confidence|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|[Organic Raspberr...|[Bag of Organic B...| 0.5984251968503937|\n",
      "|[Organic Cucumber...|[Bag of Organic B...|           0.546875|\n",
      "|[Organic Kiwi, Or...|[Bag of Organic B...| 0.5459770114942529|\n",
      "|[Organic Navel Or...|[Bag of Organic B...| 0.5412186379928315|\n",
      "|[Yellow Onions, S...|            [Banana]| 0.5357142857142857|\n",
      "|[Organic Whole St...|[Bag of Organic B...| 0.5314685314685315|\n",
      "|[Organic Navel Or...|[Bag of Organic B...| 0.5283018867924528|\n",
      "|[Organic Raspberr...|[Bag of Organic B...|  0.521099116781158|\n",
      "|[Organic D'Anjou ...|[Bag of Organic B...| 0.5170454545454546|\n",
      "|[Organic Unsweete...|[Bag of Organic B...| 0.5141065830721003|\n",
      "|[Organic Broccoli...|[Bag of Organic B...| 0.5048231511254019|\n",
      "|[Organic Lemon, O...|[Bag of Organic B...| 0.4989106753812636|\n",
      "|[Organic Hass Avo...|[Bag of Organic B...|0.49393939393939396|\n",
      "|[Organic Fuji App...|            [Banana]| 0.4915254237288136|\n",
      "|[Honeycrisp Apple...|            [Banana]| 0.4868421052631579|\n",
      "|[Organic Large Ex...|[Bag of Organic B...| 0.4838709677419355|\n",
      "|[Organic Gala App...|[Bag of Organic B...| 0.4837905236907731|\n",
      "|[Organic Navel Or...|[Bag of Organic B...| 0.4821002386634845|\n",
      "|[Organic Kiwi, Or...|[Bag of Organic B...| 0.4792332268370607|\n",
      "|[Organic Carrot B...|[Bag of Organic B...|0.47315436241610737|\n",
      "+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlDF = [antecedent (if): array<string>, consequent (then): array<string> ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[antecedent (if): array<string>, consequent (then): array<string> ... 1 more field]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlDF = spark.sql(\"select antecedent as `antecedent (if)`, consequent as `consequent (then)`, confidence from ifThen order by confidence desc limit 20\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: CSV data source does not support array<string> data type.;\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.DataSourceUtils$$anonfun$verifySchema$1.apply(DataSourceUtils.scala:69)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSourceUtils$$anonfun$verifySchema$1.apply(DataSourceUtils.scala:67)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifySchema(DataSourceUtils.scala:67)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifyWriteSchema(DataSourceUtils.scala:34)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:100)\n",
       "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
       "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlDF.write.format(\"csv\").mode(\"overwrite\").option(\"sep\",\",\").save(\"ar.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
